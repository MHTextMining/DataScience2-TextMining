[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Datascience2 - TextMining",
    "section": "",
    "text": "This is a Quarto website.\nWie schnell bist du?\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Informationen",
    "section": "",
    "text": "Modul\nData Science 2\n\n\nDatum\n10.02.2023\n\n\nAutor\nMarkus Häfner\n\n\nInstitution\nHochschule Ansbach\n\n\nBetreuer\nProf. Dr. habil. Sebastian Sauer"
  },
  {
    "objectID": "index.html#pakete-laden",
    "href": "index.html#pakete-laden",
    "title": "Datascience2 - TextMining",
    "section": "Pakete laden",
    "text": "Pakete laden\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6     ✔ purrr   0.3.4\n✔ tibble  3.1.7     ✔ dplyr   1.0.9\n✔ tidyr   1.2.0     ✔ stringr 1.4.0\n✔ readr   2.1.2     ✔ forcats 0.5.1\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(tokenizers)\n\nWarning: Paket 'tokenizers' wurde unter R Version 4.2.2 erstellt\n\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(hcandersenr)\nlibrary(SnowballC)  \nlibrary(lsa)  \n\nWarning: Paket 'lsa' wurde unter R Version 4.2.2 erstellt\n\nlibrary(easystats)  \n\n# Attaching packages: easystats 0.5.0 (red = needs update)\n✖ insight     0.17.1    ✖ datawizard  0.4.1  \n✖ bayestestR  0.12.1    ✖ performance 0.9.1  \n✖ parameters  0.18.1    ✖ effectsize  0.7.0  \n✖ modelbased  0.8.1.1   ✖ correlation 0.8.1.1\n✖ see         0.7.1.1   ✖ report      0.5.1.2\n\nRestart the R-Session and update packages in red with 'easystats::easystats_update()'.\n\nlibrary(textclean)  \n\nWarning: Paket 'textclean' wurde unter R Version 4.2.2 erstellt\n\nlibrary(tidyverse)\nlibrary(quanteda)\n\nWarning: Paket 'quanteda' wurde unter R Version 4.2.2 erstellt\n\n\nWarning in .recacheSubclasses(def@className, def, env): Nicht definierte\nSubklasse \"unpackedMatrix\" von Klasse \"mMatrix\"; Definition nicht aktualisiert\n\n\nWarning in .recacheSubclasses(def@className, def, env): Nicht definierte\nSubklasse \"unpackedMatrix\" von Klasse \"replValueSp\"; Definition nicht\naktualisiert\n\n\nPackage version: 3.2.3\nUnicode version: 13.0\nICU version: 69.1\nParallel computing: 8 of 8 threads used.\nSee https://quanteda.io for tutorials and examples.\n\nlibrary(wordcloud)\n\nWarning: Paket 'wordcloud' wurde unter R Version 4.2.2 erstellt\n\n\nLade nötiges Paket: RColorBrewer\n\nlibrary(SnowballC)\nlibrary(tidyverse)\nlibrary(easystats)\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.0.0 ──\n✔ broom        1.0.0     ✔ rsample      1.0.0\n✔ dials        1.0.0     ✔ tune         1.0.0\n✔ infer        1.0.2     ✔ workflows    1.0.0\n✔ modeldata    1.0.0     ✔ workflowsets 1.0.0\n✔ parsnip      1.0.0     ✔ yardstick    1.0.0\n✔ recipes      1.0.1     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard()        masks purrr::discard()\n✖ datawizard::extract()    masks tidyr::extract()\n✖ dplyr::filter()          masks stats::filter()\n✖ recipes::fixed()         masks stringr::fixed()\n✖ yardstick::get_weights() masks insight::get_weights()\n✖ dplyr::lag()             masks stats::lag()\n✖ yardstick::mae()         masks performance::mae()\n✖ parsnip::null_model()    masks insight::null_model()\n✖ infer::p_value()         masks parameters::p_value()\n✖ tune::parameters()       masks dials::parameters(), parameters::parameters()\n✖ yardstick::rmse()        masks performance::rmse()\n✖ dials::smoothness()      masks datawizard::smoothness()\n✖ yardstick::spec()        masks readr::spec()\n✖ recipes::step()          masks stats::step()\n• Search for functions across packages at https://www.tidymodels.org/find/\n\nlibrary(tidytext)\nlibrary(textrecipes)\n\nWarning: Paket 'textrecipes' wurde unter R Version 4.2.2 erstellt\n\nlibrary(lsa) \nlibrary(discrim)  \n\nWarning: Paket 'discrim' wurde unter R Version 4.2.2 erstellt\n\n\n\nAttache Paket: 'discrim'\n\nDas folgende Objekt ist maskiert 'package:dials':\n\n    smoothness\n\nDas folgende Objekt ist maskiert 'package:datawizard':\n\n    smoothness\n\nlibrary(naivebayes)\n\nWarning: Paket 'naivebayes' wurde unter R Version 4.2.2 erstellt\n\n\nnaivebayes 0.9.7 loaded\n\nlibrary(tictoc)  \n\nWarning: Paket 'tictoc' wurde unter R Version 4.2.2 erstellt\n\nlibrary(fastrtext)  \n\n\nAttache Paket: 'fastrtext'\n\nDas folgende Objekt ist maskiert 'package:insight':\n\n    get_parameters\n\nlibrary(remoji)  \nlibrary(tokenizers)  \nlibrary(pradadata)"
  },
  {
    "objectID": "index.html#daten-laden",
    "href": "index.html#daten-laden",
    "title": "Datascience2 - TextMining",
    "section": "Daten laden",
    "text": "Daten laden\n\nd_raw <- \n  data_read(\"C:/Users/marku/Desktop/GermEval-2018-Data-master/germeval2018.training.txt\",\n         header = FALSE)\n\nWarning in data.table::fread(input = path, ...): Found and resolved improper\nquoting out-of-sample. First healed line 111: <<\"Edel sei der Mensch, hilfreich\nund gut\" - Nicht eine dieser Charaktereigenschaften kann Merkel für sich\nbeanspruchen. OTHER OTHER>>. If the fields are not quoted (e.g. field separator\ndoes not appear within any field), try quote=\"\" to avoid this warning.\n\n\n\nnames(d_raw) <- c(\"text\", \"c1\", \"c2\")\n\n\nd_raw %>% \n  count(c1)\n\n       c1    n\n1 OFFENSE 1688\n2   OTHER 3321\n\n\n\nd_raw %>% \n  count(c2)\n\n         c2    n\n1     ABUSE 1022\n2    INSULT  595\n3     OTHER 3321\n4 PROFANITY   71\n\n\n\nd_raw %>% \n  filter(c1 == \"OTHER\", c2 == \"OTHER\") %>% \n  nrow() / nrow(d_raw)\n\n[1] 0.6630066\n\n\n\nd_raw %>% \n  filter(c1 != \"OTHER\", c2 != \"OTHER\") %>% \n  nrow() / nrow(d_raw)\n\n[1] 0.3369934\n\n\n\nd1 <-\n  d_raw %>% \n  mutate(id = as.character(1:nrow(.)))"
  },
  {
    "objectID": "index.html#textlänge",
    "href": "index.html#textlänge",
    "title": "Datascience2 - TextMining",
    "section": "Textlänge",
    "text": "Textlänge\n\nd2 <-\n  d1 %>% \n  mutate(text_length = str_length(text))\n\nhead(d2)\n\n                                                                                                                                                                                                                                                                                          text\n1                                                                                                                                                                                @corinnamilborn Liebe Corinna, wir würden dich gerne als Moderatorin für uns gewinnen! Wärst du begeisterbar?\n2                                                                                                                                               @Martin28a Sie haben ja auch Recht. Unser Tweet war etwas missverständlich. Dass das BVerfG Sachleistungen nicht ausschließt, kritisieren wir.\n3                                                                                                                                                                                                                        @ahrens_theo fröhlicher gruß aus der schönsten stadt der welt theo ⚓️\n4                                                                                                                                                 @dushanwegner Amis hätten alles und jeden gewählt...nur Hillary wollten sie nicht und eine Fortsetzung von Obama-Politik erst recht nicht..!\n5                                                                                                                                                     @spdde kein verläßlicher Verhandlungspartner. Nachkarteln nach den Sondierzngsgesprächen - schickt diese Stümper #SPD in die Versenkung.\n6 @Dirki_M Ja, aber wo widersprechen die Zahlen denn denen, die im von uns verlinkten Artikel stehen? In unserem Tweet geht es rein um subs. Geschützte. 2017 ist der gesamte Familiennachzug im Vergleich zu 2016 - die Zahlen, die Hr. Brandner bemüht - übrigens leicht rückläufig gewesen.\n       c1     c2 id text_length\n1   OTHER  OTHER  1         109\n2   OTHER  OTHER  2         142\n3   OTHER  OTHER  3          69\n4   OTHER  OTHER  4         140\n5 OFFENSE INSULT  5         136\n6   OTHER  OTHER  6         284"
  },
  {
    "objectID": "index.html#sentimentanalyse",
    "href": "index.html#sentimentanalyse",
    "title": "Datascience2 - TextMining",
    "section": "Sentimentanalyse",
    "text": "Sentimentanalyse\n\nsentiws <- read_csv(\"https://osf.io/x89wq/?action=download\")\n\nRows: 3468 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): neg_pos, word, inflections\ndbl (1): value\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nd2_long <-\n  d2 %>% \n  unnest_tokens(input = text, output = token)\n\nhead(d2_long)\n\n     c1    c2 id text_length          token\n1 OTHER OTHER  1         109 corinnamilborn\n2 OTHER OTHER  1         109          liebe\n3 OTHER OTHER  1         109        corinna\n4 OTHER OTHER  1         109            wir\n5 OTHER OTHER  1         109         würden\n6 OTHER OTHER  1         109           dich\n\n\n\nd2_long_senti <- \n  d2_long %>%  \n  inner_join(sentiws %>% select(-inflections), by = c(\"token\" = \"word\"))\n\nhead(d2_long)\n\n     c1    c2 id text_length          token\n1 OTHER OTHER  1         109 corinnamilborn\n2 OTHER OTHER  1         109          liebe\n3 OTHER OTHER  1         109        corinna\n4 OTHER OTHER  1         109            wir\n5 OTHER OTHER  1         109         würden\n6 OTHER OTHER  1         109           dich\n\n\n\nd2_sentis <-\n  d2_long_senti %>% \n  group_by(id, neg_pos) %>% \n  summarise(senti_avg = mean(value))\n\n`summarise()` has grouped output by 'id'. You can override using the `.groups`\nargument.\n\nhead(d2_sentis)\n\n# A tibble: 6 × 3\n# Groups:   id [6]\n  id    neg_pos senti_avg\n  <chr> <chr>       <dbl>\n1 1     pos        0.004 \n2 1012  neg       -0.209 \n3 1013  neg       -0.042 \n4 1015  neg       -0.0048\n5 1021  pos        0.0845\n6 1024  neg       -0.479 \n\n\n\nd2_sentis_wide <-\n  d2_sentis %>% \n  pivot_wider(names_from = \"neg_pos\", values_from = \"senti_avg\")\n\nd2_sentis_wide %>% head()\n\n# A tibble: 6 × 3\n# Groups:   id [6]\n  id        pos     neg\n  <chr>   <dbl>   <dbl>\n1 1      0.004  NA     \n2 1012  NA      -0.209 \n3 1013  NA      -0.042 \n4 1015  NA      -0.0048\n5 1021   0.0845 NA     \n6 1024   0.004  -0.479 \n\n\n\nd3 <-\n  d2 %>% \n  full_join(d2_sentis_wide)\n\nJoining, by = \"id\"\n\nhead(d3)\n\n                                                                                                                                                                                                                                                                                          text\n1                                                                                                                                                                                @corinnamilborn Liebe Corinna, wir würden dich gerne als Moderatorin für uns gewinnen! Wärst du begeisterbar?\n2                                                                                                                                               @Martin28a Sie haben ja auch Recht. Unser Tweet war etwas missverständlich. Dass das BVerfG Sachleistungen nicht ausschließt, kritisieren wir.\n3                                                                                                                                                                                                                        @ahrens_theo fröhlicher gruß aus der schönsten stadt der welt theo ⚓️\n4                                                                                                                                                 @dushanwegner Amis hätten alles und jeden gewählt...nur Hillary wollten sie nicht und eine Fortsetzung von Obama-Politik erst recht nicht..!\n5                                                                                                                                                     @spdde kein verläßlicher Verhandlungspartner. Nachkarteln nach den Sondierzngsgesprächen - schickt diese Stümper #SPD in die Versenkung.\n6 @Dirki_M Ja, aber wo widersprechen die Zahlen denn denen, die im von uns verlinkten Artikel stehen? In unserem Tweet geht es rein um subs. Geschützte. 2017 ist der gesamte Familiennachzug im Vergleich zu 2016 - die Zahlen, die Hr. Brandner bemüht - übrigens leicht rückläufig gewesen.\n       c1     c2 id text_length   pos     neg\n1   OTHER  OTHER  1         109 0.004      NA\n2   OTHER  OTHER  2         142    NA -0.3466\n3   OTHER  OTHER  3          69    NA      NA\n4   OTHER  OTHER  4         140    NA      NA\n5 OFFENSE INSULT  5         136    NA      NA\n6   OTHER  OTHER  6         284 0.004 -0.2042"
  },
  {
    "objectID": "index.html#profanities",
    "href": "index.html#profanities",
    "title": "Datascience2 - TextMining",
    "section": "Profanities",
    "text": "Profanities\n\nprofanities1 <- data_read(\"https://raw.githubusercontent.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words/master/de\", format = \",\", header = FALSE)\n\nReading data...\n\n\nPreparing data... Almost there!\n\n\n\nprofanities2 <- \n  schimpfwoerter %>% \n  mutate_all(str_to_lower) %>% \n  rename(V1 = \"word\")\n\n\nprofanities <-\n  profanities1 %>% \n  bind_rows(profanities2) %>% \n  distinct()\n\nnrow(profanities)\n\n[1] 6208\n\n\nDistinct Funktion beseitigt die doppelten Schimpfwörter\n\nd_profanity <- \nd2_long %>% \n  select(id, token) %>% \n  mutate(profanity = token %in% profanities$word)\n\n\nd_profanity %>% \n  count(profanity)\n\n  profanity      n\n1     FALSE 100217\n\n\n\nd_profanity2 <-\n  d_profanity %>% \n  group_by(id) %>% \n  summarise(profanity_n = sum(profanity))\n\nhead(d_profanity2)\n\n# A tibble: 6 × 2\n  id    profanity_n\n  <chr>       <int>\n1 1               0\n2 10              0\n3 100             0\n4 1000            0\n5 1001            0\n6 1002            0\n\n\n\nd_main <-\n  d3 %>% \n  full_join(d_profanity2)\n\nJoining, by = \"id\""
  },
  {
    "objectID": "index.html#emojis",
    "href": "index.html#emojis",
    "title": "Datascience2 - TextMining",
    "section": "Emojis",
    "text": "Emojis\n\nemj <- emoji(list_emoji(), pad = FALSE)\n\nhead(emj)\n\n[1] \"😄\" \"😃\" \"😀\" \"😊\" \"☺️\"  \"😉\"\n\n\n\nwild_emojis <- \n  c(\n    emoji(find_emoji(\"gun\")),\n    emoji(find_emoji(\"bomb\")),\n    emoji(find_emoji(\"fist\")),\n    emoji(find_emoji(\"knife\"))[1],\n    emoji(find_emoji(\"ambulance\")),\n    emoji(find_emoji(\"fist\")),\n    emoji(find_emoji(\"skull\")),\n    \"☠️\",     \"🗑\",       \"😠\",    \"👹\",    \"💩\" ,\n    \"🖕\",    \"👎️\",\n    emoji(find_emoji(\"middle finger\")),    \"😡\",    \"🤢\",    \"🤮\",  \n    \"😖\",    \"😣\",    \"😩\",    \"😨\",    \"😝\",    \"😳\",    \"😬\",    \"😱\",    \"😵\",\n       \"😤\",    \"🤦‍♀️\",    \"🤦‍\"\n  )\n\n\nwild_emojis_df <-\n  tibble(emoji = wild_emojis)\n\nsave(wild_emojis_df, file = \"C:/Users/marku/Desktop/wild_emojis.RData\")"
  },
  {
    "objectID": "index.html#stemming",
    "href": "index.html#stemming",
    "title": "Datascience2 - TextMining",
    "section": "Stemming",
    "text": "Stemming"
  },
  {
    "objectID": "index.html#data-split",
    "href": "index.html#data-split",
    "title": "Datascience2 - TextMining",
    "section": "Data Split",
    "text": "Data Split\n\nd_split <- initial_split(d1, strata = c1)\n\nd_train <- training(d_split)\nd_test <- testing(d_split)"
  },
  {
    "objectID": "index.html#rezept-1",
    "href": "index.html#rezept-1",
    "title": "Datascience2 - TextMining",
    "section": "Rezept 1",
    "text": "Rezept 1\n\nrec1 <- recipe(c1 ~ ., data = select(d_train, text, c1, id)) %>%\n  update_role(id, new_role = \"id\") %>%\n  step_tokenize(text) %>%\n  step_stopwords(text, language = \"de\", stopword_source = \"snowball\") %>%\n  step_stem(text) %>%\n  step_tokenfilter(text, max_tokens = 1e2) %>%\n  step_tfidf(text) %>%\n  step_normalize(all_numeric_predictors())\n\nrec1\n\nRecipe\n\nInputs:\n\n      role #variables\n        id          1\n   outcome          1\n predictor          1\n\nOperations:\n\nTokenization for text\nStop word removal for text\nStemming for text\nText filtering for text\nTerm frequency-inverse document frequency with text\nCentering and scaling for all_numeric_predictors()\n\n\n\nrec1_prep <- prep(rec1)\n\n\nrec1_bake <- bake(rec1_prep, new_data = NULL)\n\nhead(rec1_bake)\n\n# A tibble: 6 × 102\n  id    c1      tfidf_text__macmik tfidf_text_2 tfidf_text_ab tfidf_text_afd\n  <fct> <fct>                <dbl>        <dbl>         <dbl>          <dbl>\n1 7     OFFENSE             -0.151       -0.104        -0.103         -0.155\n2 10    OFFENSE             -0.151       -0.104        -0.103         -0.155\n3 12    OFFENSE             -0.151       -0.104        -0.103         -0.155\n4 17    OFFENSE             -0.151       -0.104        -0.103         -0.155\n5 27    OFFENSE             -0.151       -0.104        -0.103         -0.155\n6 42    OFFENSE             -0.151       -0.104        -0.103         -0.155\n# … with 96 more variables: tfidf_text_amp <dbl>, tfidf_text_anna_iina <dbl>,\n#   tfidf_text_athinamala <dbl>, tfidf_text_beim <dbl>,\n#   tfidf_text_besser <dbl>, tfidf_text_bild <dbl>, tfidf_text_cdu <dbl>,\n#   tfidf_text_charlie_silv <dbl>, tfidf_text_d <dbl>, tfidf_text_dafür <dbl>,\n#   tfidf_text_dank <dbl>, tfidf_text_dass <dbl>, tfidf_text_deutsch <dbl>,\n#   tfidf_text_deutschen <dbl>, tfidf_text_deutschland <dbl>,\n#   tfidf_text_dumm <dbl>, tfidf_text_eigentlich <dbl>, …"
  },
  {
    "objectID": "index.html#model-1",
    "href": "index.html#model-1",
    "title": "Datascience2 - TextMining",
    "section": "Model 1",
    "text": "Model 1\n\nnb_spec <- naive_Bayes() %>%\n  set_mode(\"classification\") %>%\n  set_engine(\"naivebayes\")\n\nnb_spec\n\nNaive Bayes Model Specification (classification)\n\nComputational engine: naivebayes \n\n\n\nset.seed(13)\ncv_folds <- vfold_cv(d_train)"
  },
  {
    "objectID": "index.html#workflow-1",
    "href": "index.html#workflow-1",
    "title": "Datascience2 - TextMining",
    "section": "workflow 1",
    "text": "workflow 1\n\nwf1 <- workflow() %>%\n  add_recipe(rec1) %>%\n  add_model(nb_spec)\n\nwf1\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: naive_Bayes()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n6 Recipe Steps\n\n• step_tokenize()\n• step_stopwords()\n• step_stem()\n• step_tokenfilter()\n• step_tfidf()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nNaive Bayes Model Specification (classification)\n\nComputational engine: naivebayes \n\n\n\nFit\n\nfit1 <- fit_resamples(wf1, cv_folds, control = control_resamples(save_pred = TRUE))\n\nWarning: Paket 'stopwords' wurde unter R Version 4.2.2 erstellt\n\n\n\n\nPerformance\n\nwf1_performance <- collect_metrics(fit1)\n\nwf1_performance\n\n# A tibble: 2 × 6\n  .metric  .estimator  mean     n std_err .config             \n  <chr>    <chr>      <dbl> <int>   <dbl> <chr>               \n1 accuracy binary     0.663    10 0.00974 Preprocessor1_Model1\n2 roc_auc  binary     0.605    10 0.00583 Preprocessor1_Model1\n\n\n\nwf_preds <- collect_predictions(fit1)\n\nwf_preds %>%\n  group_by(id) %>%\n  roc_curve(truth = c1, .pred_OFFENSE) %>%\n  autoplot()"
  },
  {
    "objectID": "index.html#nullmodell",
    "href": "index.html#nullmodell",
    "title": "Datascience2 - TextMining",
    "section": "Nullmodell",
    "text": "Nullmodell\n\nnull_classification <- parsnip::null_model() %>%\n  set_engine(\"parsnip\") %>%\n  set_mode(\"classification\")\n\n\nResampling\n\nnull_rs <- workflow() %>%\n  add_recipe(rec1) %>%\n  add_model(null_classification) %>%\n  fit_resamples(cv_folds)\n\n\n\nPerformance\n\nnull_rs %>%\n  collect_metrics()\n\n# A tibble: 2 × 6\n  .metric  .estimator  mean     n std_err .config             \n  <chr>    <chr>      <dbl> <int>   <dbl> <chr>               \n1 accuracy binary     0.663    10 0.00865 Preprocessor1_Model1\n2 roc_auc  binary     0.5      10 0       Preprocessor1_Model1\n\nshow_best(null_rs)\n\nWarning: No value of `metric` was given; metric 'roc_auc' will be used.\n\n\n# A tibble: 1 × 6\n  .metric .estimator  mean     n std_err .config             \n  <chr>   <chr>      <dbl> <int>   <dbl> <chr>               \n1 roc_auc binary       0.5    10       0 Preprocessor1_Model1"
  },
  {
    "objectID": "index.html#workflow-2",
    "href": "index.html#workflow-2",
    "title": "Datascience2 - TextMining",
    "section": "Workflow 2",
    "text": "Workflow 2\n\ndoParallel::registerDoParallel()\n\ncores <- parallel::detectCores(logical = TRUE)\n\nlasso_spec <- logistic_reg(penalty = tune(), mixture = 1) %>%\n  set_mode(\"classification\") %>%\n  set_engine(\"glmnet\", num.threads = cores)\n\nlasso_spec\n\nLogistic Regression Model Specification (classification)\n\nMain Arguments:\n  penalty = tune()\n  mixture = 1\n\nEngine-Specific Arguments:\n  num.threads = cores\n\nComputational engine: glmnet \n\n\n\nlambda_grid <- grid_regular(penalty(), levels = 30)\n\n\nwf2 <- workflow() %>%\n  add_recipe(rec1) %>%\n  add_model(lasso_spec)\n\nwf2\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: logistic_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n6 Recipe Steps\n\n• step_tokenize()\n• step_stopwords()\n• step_stem()\n• step_tokenfilter()\n• step_tfidf()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLogistic Regression Model Specification (classification)\n\nMain Arguments:\n  penalty = tune()\n  mixture = 1\n\nEngine-Specific Arguments:\n  num.threads = cores\n\nComputational engine: glmnet \n\n\n\nTune & Fit\n\nset.seed(2246)\n\ntic()\n\nfit2 <- tune_grid(wf2, cv_folds, grid = lambda_grid, control = control_resamples(save_pred = TRUE))\n\ntoc()\n\n19.36 sec elapsed\n\nfit2\n\n# Tuning results\n# 10-fold cross-validation \n# A tibble: 10 × 5\n   splits             id     .metrics          .notes           .predictions\n   <list>             <chr>  <list>            <list>           <list>      \n 1 <split [3380/376]> Fold01 <tibble [60 × 5]> <tibble [0 × 3]> <tibble>    \n 2 <split [3380/376]> Fold02 <tibble [60 × 5]> <tibble [0 × 3]> <tibble>    \n 3 <split [3380/376]> Fold03 <tibble [60 × 5]> <tibble [0 × 3]> <tibble>    \n 4 <split [3380/376]> Fold04 <tibble [60 × 5]> <tibble [0 × 3]> <tibble>    \n 5 <split [3380/376]> Fold05 <tibble [60 × 5]> <tibble [0 × 3]> <tibble>    \n 6 <split [3380/376]> Fold06 <tibble [60 × 5]> <tibble [0 × 3]> <tibble>    \n 7 <split [3381/375]> Fold07 <tibble [60 × 5]> <tibble [0 × 3]> <tibble>    \n 8 <split [3381/375]> Fold08 <tibble [60 × 5]> <tibble [0 × 3]> <tibble>    \n 9 <split [3381/375]> Fold09 <tibble [60 × 5]> <tibble [0 × 3]> <tibble>    \n10 <split [3381/375]> Fold10 <tibble [60 × 5]> <tibble [0 × 3]> <tibble>    \n\n\n\n\nPerformance\n\ncollect_metrics(fit2) %>%\n  filter(.metric == \"roc_auc\") %>%\n  slice_max(mean, n = 7)\n\n# A tibble: 24 × 7\n    penalty .metric .estimator  mean     n std_err .config              \n      <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n 1 8.53e- 3 roc_auc binary     0.643    10  0.0107 Preprocessor1_Model24\n 2 3.86e- 3 roc_auc binary     0.641    10  0.0105 Preprocessor1_Model23\n 3 7.88e- 4 roc_auc binary     0.639    10  0.0106 Preprocessor1_Model21\n 4 1.89e- 2 roc_auc binary     0.638    10  0.0114 Preprocessor1_Model25\n 5 1.74e- 3 roc_auc binary     0.638    10  0.0105 Preprocessor1_Model22\n 6 3.56e- 4 roc_auc binary     0.638    10  0.0106 Preprocessor1_Model20\n 7 1   e-10 roc_auc binary     0.638    10  0.0105 Preprocessor1_Model01\n 8 2.21e-10 roc_auc binary     0.638    10  0.0105 Preprocessor1_Model02\n 9 4.89e-10 roc_auc binary     0.638    10  0.0105 Preprocessor1_Model03\n10 1.08e- 9 roc_auc binary     0.638    10  0.0105 Preprocessor1_Model04\n# … with 14 more rows\n\n\n\nautoplot(fit2)\n\n\n\n\n\nfit2 %>%\n  show_best(\"roc_auc\")\n\n# A tibble: 5 × 7\n   penalty .metric .estimator  mean     n std_err .config              \n     <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n1 0.00853  roc_auc binary     0.643    10  0.0107 Preprocessor1_Model24\n2 0.00386  roc_auc binary     0.641    10  0.0105 Preprocessor1_Model23\n3 0.000788 roc_auc binary     0.639    10  0.0106 Preprocessor1_Model21\n4 0.0189   roc_auc binary     0.638    10  0.0114 Preprocessor1_Model25\n5 0.00174  roc_auc binary     0.638    10  0.0105 Preprocessor1_Model22\n\n\n\nchosen_auc <- fit2 %>%\n  select_by_one_std_err(metric = \"roc_auc\", -penalty)\n\n\n\nFinalize\n\nwf2_final <- finalize_workflow(wf2, chosen_auc)\n\nwf2_final\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: logistic_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n6 Recipe Steps\n\n• step_tokenize()\n• step_stopwords()\n• step_stem()\n• step_tokenfilter()\n• step_tfidf()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLogistic Regression Model Specification (classification)\n\nMain Arguments:\n  penalty = 0.018873918221351\n  mixture = 1\n\nEngine-Specific Arguments:\n  num.threads = cores\n\nComputational engine: glmnet \n\n\n\nfit2_final_train <- fit(wf2_final, d_train)\n\n\nfit2_final_train %>%\n  extract_fit_parsnip() %>%\n  tidy() %>%\n  arrange(-abs(estimate)) %>%\n  head()\n\nLade nötiges Paket: Matrix\n\n\n\nAttache Paket: 'Matrix'\n\n\nDie folgenden Objekte sind maskiert von 'package:tidyr':\n\n    expand, pack, unpack\n\n\nLoaded glmnet 4.1-4\n\n\n# A tibble: 6 × 3\n  term              estimate penalty\n  <chr>                <dbl>   <dbl>\n1 (Intercept)         0.684   0.0189\n2 tfidf_text_merkel  -0.163   0.0189\n3 tfidf_text_dumm    -0.161   0.0189\n4 tfidf_text_lbr     -0.107   0.0189\n5 tfidf_text_islam   -0.0720  0.0189\n6 tfidf_text_schulz  -0.0578  0.0189\n\n\n\nfit2_final_test <- last_fit(wf2_final, d_split)\n\ncollect_metrics(fit2_final_test)\n\n# A tibble: 2 × 4\n  .metric  .estimator .estimate .config             \n  <chr>    <chr>          <dbl> <chr>               \n1 accuracy binary         0.681 Preprocessor1_Model1\n2 roc_auc  binary         0.611 Preprocessor1_Model1"
  }
]