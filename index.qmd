---
title: "Analysebericht"
author: "Markus H√§fner"
date: "02.10.2023"
format:
  html:
    toc: true
---


# Setup

## Pakete laden

```{r, results='hide', warning=FALSE, message=FALSE}
library(tidyverse)
library(tokenizers)
library(tidyverse)
library(tidytext)
library(hcandersenr)
library(SnowballC)  
library(lsa)  
library(easystats)  
library(textclean)  
library(tidyverse)
library(quanteda)
library(wordcloud)
library(SnowballC)
library(tidyverse)
library(easystats)
library(tidymodels)
library(tidytext)
library(textrecipes)
library(lsa) 
library(discrim)  
library(naivebayes)
library(tictoc)  
library(fastrtext)  
library(remoji)  
library(tokenizers)  
library(pradadata)
library(knitr)
library(parsnip)
```

## Training - Data

### Load data

```{r}
d_train <- 
  data_read("data/germeval2018.training.txt",
         header = FALSE)

kable(head(d_train))
```
### Renaming Columns

```{r}
names(d_train) <- c("text", "c1", "c2")
```

### Adding an ID Column

```{r}
d_train_id <- d_train %>% 
  mutate(id = row_number()) 
```

## Testing - Data

### Load data

```{r}
d_test <- 
  data_read("data/germeval2018.test.txt",
         header = FALSE)

head(d_test)
```
### Renaming Columns

```{r}
names(d_test) <- c("text", "c1", "c2")
```

### Adding an ID Column

```{r}
d_test_id <- d_test %>% 
  mutate(id = row_number()) 
```

# Exploratory data analysis

## Classifier 1

```{r}
kable(d_train_id %>% 
  count(c1))
```
## Classifier 2
```{r}
kable(d_train_id %>% 
  count(c2))
```

```{r}
d_train %>% 
  filter(c1 == "OTHER" & c2 == "OTHER") %>%
  nrow() / nrow(d_train)
```
```{r}
d_train %>% 
  filter(c1 == "OFFENSE" & c2 == "ABUSE") %>%
  nrow() / nrow(d_train)
```

```{r}
d_train %>% 
  filter(c1 == "OFFENSE" & c2 == "INSULT") %>%
  nrow() / nrow(d_train)
```

```{r}
d_train %>% 
  filter(c1 == "OFFENSE" & c2 == "PROFANITY") %>%
  nrow() / nrow(d_train)
```

```{r}
d_train %>% 
  filter(c1 == "OFFENSE" & c2 == "OTHER") %>%
  nrow() / nrow(d_train)
```
-> Every tweet that has been classified as "Offense" is categorizied in either of the three categories PROFANITY, ABUSE or INSULT


# Feature Engineering

### Adding Text_length as a variable

```{r}
d_train_tl <-
  d_train_id %>% 
  mutate(text_length = str_length(text))

kable(head(d_train_tl))
```

## Sentiment analysis

```{r}
sentiments <- read_csv("data/sentiments.csv")
```

```{r}
d_train_unnest <-
  d_train_tl %>% 
  unnest_tokens(input = text, output = token)

kable(head(d_train_unnest))
```

```{r}
d_train_senti <- 
  d_train_unnest %>%  
  inner_join(sentiments %>% select(-inflections), by = c("token" = "word"))

kable(head(d_train_senti))
```

```{r}
train_sentiments <-
  d_train_senti %>% 
  group_by(id, neg_pos) %>% 
  summarise(mean = mean(value))
```

```{r}
train_sentiments_spread <-
  train_sentiments %>% 
  pivot_wider(names_from = "neg_pos", values_from = "mean")

kable(head(train_sentiments_spread))
```

```{r}
d_train_senti <-
  d_train_tl %>% 
  full_join(train_sentiments_spread)

kable(head(d_train_senti))
```

## Profanities

```{r}
profanities1 <- 
  data_read("data/profanities.txt",
         header = FALSE)
```

```{r}
 profanities2 <- 
   schimpfwoerter %>% 
   mutate_all(str_to_lower) %>% 
   rename(V1 = "word")
```

```{r}
profanities3 <- 
  data_read("data/profanities_en.txt",
         header = FALSE)
```
 from http://www.cs.cmu.edu/~biglou/resources/
```{r}
profanities <-
  profanities1 %>% 
  bind_rows(profanities2) %>%
  bind_rows(profanities3) %>%
  distinct()

nrow(profanities)
```

Distinct Funktion beseitigt die doppelten Schimpfw√∂rter

```{r}
d_train_prof <- 
d_train_unnest %>% 
  select(id, token) %>% 
  mutate(profanity = token %in% profanities$V1)
```

```{r}
kable(d_train_prof %>% 
  count(profanity))
```

```{r}
d_train_prof %>% 
  group_by(id) %>% 
  summarise(profanity_n = sum(profanity))

kable(head(d_train_prof))
```

```{r}
d_train2 <-
  d_train_senti %>% 
  full_join(d_train_prof)
```

## Emojis

```{r}
emj <- emoji(list_emoji(), pad = FALSE)

head(emj)
```

```{r}
wild_emojis <- 
  c(
    emoji(find_emoji("gun")),
    emoji(find_emoji("bomb")),
    emoji(find_emoji("fist")),
    emoji(find_emoji("knife"))[1],
    emoji(find_emoji("ambulance")),
    emoji(find_emoji("fist")),
    emoji(find_emoji("skull")),
    "‚ò†Ô∏è",     "üóë",       "üò†",    "üëπ",    "üí©" ,
    "üñï",    "üëéÔ∏è",
    emoji(find_emoji("middle finger")),    "üò°",    "ü§¢",    "ü§Æ",  
    "üòñ",    "üò£",    "üò©",    "üò®",    "üòù",    "üò≥",    "üò¨",    "üò±",    "üòµ",
       "üò§",    "ü§¶‚Äç‚ôÄÔ∏è",    "ü§¶‚Äç"
  )

```

```{r}
wild_emojis_df <-
  tibble(emoji = wild_emojis)

save(wild_emojis_df, file = "data/wild_emojis.RData")
```

## Stemming

# Recipes

## Recipe 0

As a baseline recipe we are going to refrain from using advanced methods for now and just focus on a "clean" text predictions. For this we use the following methods: Removal of german stop words, word stemming and normalization of all predictors.

### Defining recipe 0

```{r}
rec0 <- recipe(c1 ~ ., data = select(d_train2, text, c1, id)) %>%
  update_role(id, new_role = "id") %>%
  step_tokenize(text) %>%
  step_stopwords(text, language = "de", stopword_source = "snowball") %>%
  step_stem(text) %>%
  step_normalize(all_numeric_predictors())

rec0
```

### Preparing/Baking recipe 0

```{r}
rec0_prep <- prep(rec0)

rec0_bake <- bake(rec0_prep, new_data = NULL)

kable(head(rec0_bake))
```

## Recipe 1

Recipe 1 applies all steps from recipe 0 and adds `step_tf`, which converts a token variable into multiple variables 
containing the token counts.
To avoid running into memory issues, we apply a restriction for the amount of tokens (n = 100) using `step_tokenfilter`. 

### Defining recipe 1

```{r}
rec1 <- recipe(c1 ~ ., data = select(d_train2, text, c1, id)) %>%
  update_role(id, new_role = "id") %>%
  step_tokenize(text) %>%
  step_stopwords(text, language = "de", stopword_source = "snowball") %>%
  step_stem(text) %>%
  step_tokenfilter(text, max_tokens = 1e2) %>%
  step_tf(text) %>%
  step_normalize(all_numeric_predictors())

rec1
```

### Preparing/Baking recipe 2

```{r}
rec1_prep <- prep(rec1)

rec1_bake <- bake(rec1_prep, new_data = NULL)

kable(head(rec1_bake))
```

## Recipe 2

In this recipe we change `step_tf` to `step_tfidf`, which results in an inverse Document Frequency of our tokens.

### Defining recipe 2

```{r}
rec2 <- recipe(c1 ~ ., data = select(d_train2, text, c1, id)) %>%
  update_role(id, new_role = "id") %>%
  step_tokenize(text) %>%
  step_stopwords(text, language = "de", stopword_source = "snowball") %>%
  step_stem(text) %>%
  step_tokenfilter(text, max_tokens = 1e2) %>%
  step_tfidf(text) %>%
  step_normalize(all_numeric_predictors())

rec2
```

### Preparing/Baking recipe 2

```{r}
rec2_prep <- prep(rec2)

rec2_bake <- bake(rec2_prep, new_data = NULL)

kable(head(rec2_bake))
```

# Models

## Model 0

```{r}
m_null <- null_model() %>%
  set_engine("parsnip") %>%
  set_mode("classification")

m_null
```

## Model 1

```{r}
m_nb <- naive_Bayes() %>%
  set_mode("classification") %>%
  set_engine("naivebayes")

m_nb
```

```{r}
set.seed(13)
cv_folds <- vfold_cv(d_train2)
```

# Workflow

```{r, echo=FALSE}
wf_set <- workflow_set(preproc = list(recipe1 = rec1, recipe2 = rec2),
                                  models = list(m_null = m_null, m_nb = m_nb),
                                  cross = TRUE)
```

```{r, echo=FALSE}
lambda_grid <- grid_regular(penalty(), levels = 30)
```

```{r, echo=FALSE}
wf_set_adjusted <- wf_set %>%
               option_add(grid = lambda_grid, id = str_match(wf_set$wflow_id, ".*m_null$")) %>%
               option_add(grid = lambda_grid, id = str_match(wf_set$wflow_id, ".*m_nb$"))
```

```{r, echo=FALSE}
wf_set_fit <- workflow_map(wf_set_adjusted, fn = "tune_grid", resamples = vfold_cv(d_train2, v = 10, strata = c1), verbose = TRUE)
autoplot(wf_set_fit)
```

```{r, echo=FALSE}
train_metrics <- wf_set_fit %>%
               collect_metrics() %>%
               filter(.metric == "roc_auc") %>%
               arrange(-mean)
```

```{r, echo=FALSE}
best_wf_id <- train_metrics %>% 
               slice_head(n = 1) %>% 
               pull(wflow_id)
```

```{r, echo=FALSE}
best_wf <- wf_set_fit %>%
               extract_workflow(best_wf_id)
```

```{r, echo=FALSE}
best_wf_fit <- wf_set_fit %>% 
               extract_workflow_set_result(best_wf_id)
```

```{r, echo=FALSE}
best_wf_finalized <- best_wf %>% 
               finalize_workflow(select_best(best_wf_fit))
```

```{r, echo=FALSE}
last_fit <- fit(best_wf_finalized, d_train2)
```

```{r, echo=FALSE}
test_predicted <- bind_cols(d_test_id, predict(last_fit, new_data = d_test_id)) %>% 
               mutate(c1 = factor(c1))
```

```{r, echo=FALSE}
test_metrics <- test_predicted %>% 
               metrics(c1, .pred_class)
```

## Workflow 1

```{r}
#wf1 <- workflow() %>%
 # add_recipe(rec1) %>%
 # add_model(nb_spec)

#wf1
```

### Fit

```{r}
#fit1 <- fit_resamples(wf1, cv_folds, control = control_resamples(save_pred = TRUE))
```

### Performance

```{r}
#wf1_performance <- collect_metrics(fit1)

#wf1_performance
```

```{r}
#wf_preds <- collect_predictions(fit1)

#wf_preds %>%
 # group_by(id) %>%
 # roc_curve(truth = c1, .pred_OFFENSE) %>%
 # autoplot()

```

## Nullmodell

```{r}
null_classification <- null_model() %>%
  set_engine("parsnip") %>%
  set_mode("classification")
```

```{r}
set.seed(42)
folds1 <- vfold_cv(d_train2)
```

### Resampling

```{r}
null_rs <- workflow() %>%
  add_recipe(rec1) %>%
  add_model(null_classification) %>%
  fit_resamples(folds1)
```

### Performance

```{r}
null_rs %>%
  collect_metrics()

show_best(null_rs)
```

## Workflow 2

```{r}
#doParallel::registerDoParallel()

#cores <- parallel::detectCores(logical = TRUE)

#lasso_spec <- logistic_reg(penalty = tune(), mixture = 1) %>%
 # set_mode("classification") %>%
 # set_engine("glmnet", num.threads = cores)

#lasso_spec
```

```{r}
#lambda_grid <- grid_regular(penalty(), levels = 30)
```

```{r}
#wf2 <- workflow() %>%
  #add_recipe(rec1) %>%
  #add_model(lasso_spec)

#wf2
```

### Tune & Fit

```{r}
#set.seed(2246)

#tic()

#fit2 <- tune_grid(wf2, cv_folds, grid = lambda_grid, control = control_resamples(save_pred = TRUE))

#toc()

#fit2
```

### Performance

```{r}
#collect_metrics(fit2) %>%
 # filter(.metric == "roc_auc") %>%
 # slice_max(mean, n = 7)
```

```{r}
#autoplot(fit2)
```

```{r}
#fit2 %>%
 # show_best("roc_auc")
```

```{r}
#chosen_auc <- fit2 %>%
 # select_by_one_std_err(metric = "roc_auc", -penalty)
```

### Finalize

```{r}
#wf2_final <- finalize_workflow(wf2, chosen_auc)

#wf2_final
```

```{r}
#fit2_final_train <- fit(wf2_final, d_train)
```

```{r}
#fit2_final_train %>%
#  extract_fit_parsnip() %>%
 # tidy() %>%
 # arrange(-abs(estimate)) %>%
 # head()
```

```{r}
#fit2_final_test <- last_fit(wf2_final, d_split)

#collect_metrics(fit2_final_test)
```
